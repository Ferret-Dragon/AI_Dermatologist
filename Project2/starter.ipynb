{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef102b53-2348-490e-8257-ac32916d5277",
   "metadata": {},
   "source": [
    "# Traffic Light Autoencoder - Starter Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb083df-4bb4-4b43-b224-d556d0ae1c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, random, subprocess, sys, zipfile\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c1c8e-394d-4206-beee-803d20a3e311",
   "metadata": {},
   "source": [
    "## Step 1: Download the zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e101a094-5e8c-4e79-95bf-2106177ef0d7",
   "metadata": {},
   "source": [
    "This snippet downloads the training dataset ZIP from the course server and saves it locally as training_dataset.zip. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e79d80-cc3b-4c73-ae70-ee65c4b501af",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://hadi.cs.virginia.edu:9000/download/train-dataset-hw2\"\n",
    "out = Path(\"training_dataset.zip\")\n",
    "\n",
    "with requests.get(url, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with out.open(\"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7848b-0b1e-4554-b9e4-3434b7287248",
   "metadata": {},
   "source": [
    "## Step 2: Provide the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f844be-c566-4598-85cd-9758e21fc2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbe1836d450>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- CONFIG ----------\n",
    "DATA_ROOT  = Path(\"data\")\n",
    "ZIP_PATH   = Path(\"training_dataset.zip\")  # provided to students\n",
    "TRAIN_DIR  = DATA_ROOT / \"training_dataset\"        # unzip target\n",
    "\n",
    "IMG_SIZE   = 256 #NOT ALLOWED TO CHANGE\n",
    "GRAYSCALE  = False\n",
    "LATENT_DIM = 32\n",
    "BATCH_SIZE = 64            # drop to 32/16 if OOM\n",
    "EPOCHS     = 20\n",
    "LR         = 2e-3\n",
    "SEED       = 42\n",
    "MIN_BOX    = 8\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LAMBDA_L1  = 0.5       # Optional: use mixed loss for sharper recon (0..1) # 0 = pure MSE, 0.5 = half MSE + half L1\n",
    "random.seed(SEED); torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a759f87-f10d-4cc0-a634-a507fb99fb18",
   "metadata": {},
   "source": [
    "## Step 3: Unzip the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03e8011-2906-4ce7-8499-9b9266a103b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping training_dataset.zip -> data/training_dataset ...\n",
      "Unzip done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- Unzip training dataset ----------\n",
    "def ensure_unzipped(zip_path: Path, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # If folder already has images, skip\n",
    "    has_images = any(p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}\n",
    "                     for p in out_dir.rglob(\"*\"))\n",
    "    if has_images:\n",
    "        print(f\"Training images already present under: {out_dir}\")\n",
    "        return\n",
    "    if not zip_path.exists():\n",
    "        raise SystemExit(f\"â—ï¸ Zip not found: {zip_path}\\n\"\n",
    "                         f\"Place training_dataset.zip at {zip_path} and rerun.\")\n",
    "    print(f\"Unzipping {zip_path} -> {out_dir} ...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(out_dir)\n",
    "    print(\"Unzip done.\")\n",
    "\n",
    "ensure_unzipped(ZIP_PATH, TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9ad4a-8233-47f8-a4e7-42d423bb3802",
   "metadata": {},
   "source": [
    "## Optional Step (Please READ)\n",
    "This zip file was created on MACOS so it may happen that a __MACOSX folder may be created inside data/training_dataset. If it is, please execute below code to delete that folder as it leads 2xthe number of images, which will hamper your performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78945e-1ab3-47f2-9c72-b2b8232b304b",
   "metadata": {},
   "source": [
    "import shutil\n",
    "import stat\n",
    "\n",
    "target = Path(\"data/training_dataset/__MACOSX\")\n",
    "\n",
    "if target.exists():\n",
    "    if target.is_dir():\n",
    "        shutil.rmtree(target)\n",
    "        print(f\"Removed: {target}\")\n",
    "    else:\n",
    "        print(f\"Exists but is not a directory: {target}\")\n",
    "else:\n",
    "    print(\"Nothing to remove.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815db0e-3078-4f06-a1ec-004a77a842f9",
   "metadata": {},
   "source": [
    "## Step 4: Collect all training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7359354a-fb5d-4b76-8ae7-9d6cab77a2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val (ALL training_dataset/ images): train=12630  val=1404\n"
     ]
    }
   ],
   "source": [
    "# ---------- Collect ALL training images (no labels needed) ----------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "def is_image_file(p: Path) -> bool:\n",
    "    return p.is_file() and p.suffix.lower() in IMG_EXTS\n",
    "\n",
    "all_train_imgs = [str(p.resolve()) for p in TRAIN_DIR.rglob(\"*\") if is_image_file(p)]\n",
    "if not all_train_imgs:\n",
    "    raise SystemExit(f\"No images found under {TRAIN_DIR}. Check your zip contents.\")\n",
    "\n",
    "random.shuffle(all_train_imgs)\n",
    "n = len(all_train_imgs); n_tr = int(0.9*n)\n",
    "train_imgs = all_train_imgs[:n_tr]\n",
    "val_imgs   = all_train_imgs[n_tr:]\n",
    "print(f\"Train/Val (ALL training_dataset/ images): train={len(train_imgs)}  val={len(val_imgs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f651a-3ff8-4963-9913-20749c986849",
   "metadata": {},
   "source": [
    "## Step 6: Get train_loader and val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733da491-a38f-4a6e-8de7-525db2b47499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Transforms / Datasets / Loaders ----------\n",
    "def fullframe_transform(img_size=IMG_SIZE, grayscale=GRAYSCALE):\n",
    "    t = [transforms.Resize((img_size, img_size), interpolation=InterpolationMode.BILINEAR)]\n",
    "    if grayscale: t.append(transforms.Grayscale(1))\n",
    "    t.append(transforms.ToTensor())  # [0,1]\n",
    "    return transforms.Compose(t)\n",
    "\n",
    "FF_TF = fullframe_transform()\n",
    "\n",
    "class FullFrameDS(Dataset):\n",
    "    def __init__(self, img_paths): self.paths = img_paths\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        p = self.paths[i]\n",
    "        with Image.open(p) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "            x  = FF_TF(im)\n",
    "        return x, 0\n",
    "\n",
    "def probe_workers():\n",
    "    try:\n",
    "        _ = DataLoader([0], num_workers=2); return 2\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "nw = probe_workers()\n",
    "train_loader = DataLoader(FullFrameDS(train_imgs), batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=nw, pin_memory=(DEVICE==\"cuda\"))\n",
    "val_loader   = DataLoader(FullFrameDS(val_imgs),   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=nw, pin_memory=(DEVICE==\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b3462-836c-4c8d-8e2f-b577af9b71a6",
   "metadata": {},
   "source": [
    "## Step 7: Define Autoencoder and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05f1b703-6c3a-4e73-9a6e-dfcf19f1218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enc(nn.Module):\n",
    "    def __init__(self, in_ch, ld):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch,   32, 4, 2, 1), nn.ReLU(True),  # /2\n",
    "            nn.Conv2d(32,      64, 4, 2, 1), nn.ReLU(True),  # /4\n",
    "            nn.Conv2d(64,     128, 4, 2, 1), nn.ReLU(True),  # /8\n",
    "            nn.Conv2d(128,    256, 4, 2, 1), nn.ReLU(True),  # /16\n",
    "        )\n",
    "        self.fc = None\n",
    "        self.ld = ld\n",
    "        self.shape = None\n",
    "    def _init_fc(self, h):\n",
    "        n, c, H, W = h.shape\n",
    "        self.shape = (c, H, W)\n",
    "        self.fc = nn.Linear(c * H * W, self.ld).to(h.device)\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        if self.fc is None: self._init_fc(h)\n",
    "        z = self.fc(h.view(h.size(0), -1))\n",
    "        return z\n",
    "\n",
    "class Dec(nn.Module):\n",
    "    def __init__(self, out_ch, ld, img_size=IMG_SIZE):\n",
    "        super().__init__()\n",
    "        self._shape = (256, img_size//16, img_size//16)\n",
    "        self.fc = nn.Linear(ld, int(np.prod(self._shape)))\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(True),  # x2\n",
    "            nn.ConvTranspose2d(128,  64, 4, 2, 1), nn.ReLU(True),  # x4\n",
    "            nn.ConvTranspose2d(64,   32, 4, 2, 1), nn.ReLU(True),  # x8\n",
    "            nn.ConvTranspose2d(32, out_ch, 4, 2, 1), nn.Sigmoid(), # x16\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        h = self.fc(z)\n",
    "        c, H, W = self._shape\n",
    "        h = h.view(z.size(0), c, H, W)\n",
    "        return self.deconv(h)\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, c, ld, img_size=IMG_SIZE):\n",
    "        super().__init__()\n",
    "        self.enc = Enc(c, ld)\n",
    "        self.dec = Dec(c, ld, img_size)\n",
    "    def forward(self, x):\n",
    "        return self.dec(self.enc(x))\n",
    "\n",
    "CHANNELS = 1 if GRAYSCALE else 3\n",
    "model = AE(CHANNELS, LATENT_DIM, IMG_SIZE).to(DEVICE)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "crit_mse = nn.MSELoss()\n",
    "crit_l1  = nn.L1Loss()\n",
    "\n",
    "def recon_loss(y, x):\n",
    "    if LAMBDA_L1 <= 0: return crit_mse(y, x)\n",
    "    return (1 - LAMBDA_L1) * crit_mse(y, x) + LAMBDA_L1 * crit_l1(y, x)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_full_mse(model, loader):\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    for x, _ in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = model(x)\n",
    "        total += crit_mse(y, x).item() * x.size(0)\n",
    "        n += x.size(0)\n",
    "    return total / max(1, n)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "ckpt_path = \"model_ts.pt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54717aec-b85b-42bc-94a5-fa13ec4ed109",
   "metadata": {},
   "source": [
    "## Step 8: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728425fd-349a-491b-8d4f-9d28e28c87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = model(x)\n",
    "        loss = recon_loss(y, x)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item() * x.size(0)\n",
    "        n     += x.size(0)\n",
    "    train_loss = total / max(1, n)\n",
    "\n",
    "    val_mse = eval_full_mse(model, val_loader) if len(val_imgs) else float(\"nan\")\n",
    "    print(f\"epoch {ep:02d} | train_recon {train_loss:.6f} | val_full_mse {val_mse:.6f}\")\n",
    "    if len(val_imgs) and val_mse < best_val:\n",
    "        best_val = val_mse\n",
    "        scripted_model = torch.jit.script(model)\n",
    "        scripted_model.save(ckpt_path)\n",
    "        print(f\"ðŸ’¾ saved {ckpt_path} (best full-image val MSE)\")\n",
    "\n",
    "if not Path(ckpt_path).exists():\n",
    "    scripted_model = torch.jit.script(model)\n",
    "    scripted_model.save(ckpt_path)\n",
    "    print(f\"ðŸ’¾ saved {ckpt_path} (final TorchScript)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22af9ecc-38dc-441e-8751-dd91b5289399",
   "metadata": {},
   "source": [
    "## Step 9: Check Error on training dataset provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b850c28d-0442-4ce6-87b6-9bf8cf45a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_loader = DataLoader(\n",
    "    FullFrameDS(train_imgs + val_imgs),  # entire training_dataset/\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=nw,\n",
    "    pin_memory=(DEVICE==\"cuda\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a6c0dc6-6af5-4847-85fa-17af8dba69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(ckpt_path, map_location=DEVICE)\n",
    "model.eval()\n",
    "all_full_mse   = eval_full_mse(model, all_train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d5fb730-8621-4c34-bf56-06c3f5e6d699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Full-image MSE on training data ===\n",
      "Entire training_dataset: 0.004635\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Full-image MSE on training data ===\")\n",
    "print(f\"Entire training_dataset: {all_full_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254e6a7-225a-451b-80fe-7eb91f124953",
   "metadata": {},
   "source": [
    "## Step 10: Submit to Server and Also Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ef9571b-7143-43a4-b77b-12bb0b931ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Submission received for team 'Shreejeet-Test'. Attempt #5.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Submit to server\n",
    "# -------------------------\n",
    "import requests\n",
    "def submit_model(token: str, model_path: str, server_url=\"http://hadi.cs.virginia.edu:9000\"):\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        files = {\"file\": f}\n",
    "        data = {\"token\": token}\n",
    "        response = requests.post(f\"{server_url}/submit\", data=data, files=files)\n",
    "        resp_json = response.json()\n",
    "        if \"message\" in resp_json:\n",
    "            print(f\"âœ… {resp_json['message']}\")\n",
    "        else:\n",
    "            print(f\"âŒ Submission failed: {resp_json.get('error')}\")\n",
    "\n",
    "\n",
    "# Replace with your team token\n",
    "my_token = \"your token\"\n",
    "file_name = \"your file name\"\n",
    "submit_model(my_token, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7a36c-96bc-4e0e-93f5-2d98cd350e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "#  Check status\n",
    "# -------------------------\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def check_submission_status(my_token, max_retries=3):\n",
    "    url = f\"http://hadi.cs.virginia.edu:9000/submission-status/{my_token}\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            attempts = response.json()\n",
    "            for a in attempts:\n",
    "                model_size = f\"{a['model_size']:.4f}\" if isinstance(a['model_size'], (float, int)) else \"None\"\n",
    "\n",
    "                print(f\"Attempt {a['attempt']}: Model size={model_size}, \"\n",
    "                      f\"Submitted at={a['submitted_at']}, Status={a['status']}\")\n",
    "\n",
    "            if attempts and attempts[-1]['status'].lower() == \"broken file\":\n",
    "                print(\"âš ï¸ Your model on the server is broken!\")\n",
    "            return  # success, exit function\n",
    "\n",
    "        elif response.status_code == 429:\n",
    "            # Server says rate limit exceeded\n",
    "            try:\n",
    "                error_json = response.json()\n",
    "                wait_seconds = int(error_json.get(\"error\", \"\").split()[-2])\n",
    "            except Exception:\n",
    "                wait_seconds = 15  # default fallback\n",
    "            print(f\"â³ Rate limited. Waiting {wait_seconds} seconds before retry...\")\n",
    "            time.sleep(wait_seconds + 1)  # wait a bit longer to be safe\n",
    "\n",
    "        else:\n",
    "            print(f\"âŒ Error {response.status_code}: {response.text}\")\n",
    "            return\n",
    "\n",
    "    print(\"âš ï¸ Max retries reached. Try again later.\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "check_submission_status(my_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a567917-640e-4612-8e22-c73313346cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.7.0",
   "language": "python",
   "name": "pytorch-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
