{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Knowledge Distillation for AI Dermatologist\n",
    "\n",
    "## CS 4774 Machine Learning - University of Virginia\n",
    "\n",
    "In this notebook, you'll implement knowledge distillation to improve your skin disease classifier by learning from **MedSigLIP** (from Google), a powerful medical imaging model.\n",
    "\n",
    "**Key Requirements:**\n",
    "- Student model must be < **25 MB** on disk\n",
    "- Use MedSigLIP as frozen teacher model (inference only)\n",
    "- Implement temperature-scaled knowledge distillation following Hinton et al. (2015)\n",
    "\n",
    "**Recommended Starting Point:** Use ShuffleNetV2 for your student model (~5 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Enable cudnn benchmarking for faster training with fixed input sizes\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"‚úì cuDNN benchmark enabled for faster training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Login - Run this cell first!\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Interactive login (will prompt for token)\n",
    "login()\n",
    "\n",
    "# Option 2: Direct login (replace with your token)\n",
    "# login(token=\"hf_YOUR_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# HuggingFace Login - Run this cell first!\nfrom huggingface_hub import login\n\n# Direct login with your token\nlogin(token=\"hf_YOUR_TOKEN_HERE\")  # Replace with your actual token\n\nprint(\"‚úÖ Logged in to HuggingFace\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# CONFIGURATION - Change these values to tune your model\n",
    "# =============================\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET_PATH = 'train_dataset'\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Image Processing\n",
    "IMAGE_SIZE = 224  # Image dimensions (224x224)\n",
    "NORMALIZE_MEAN = [0.485, 0.456, 0.406]  # ImageNet mean\n",
    "NORMALIZE_STD = [0.229, 0.224, 0.225]   # ImageNet std\n",
    "\n",
    "# Training Parameters\n",
    "BATCH_SIZE = 64  # Increased from 32 for faster training\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_WORKERS = 4  # Increased from 2 for faster data loading (adjust based on your CPU cores)\n",
    "\n",
    "# Data Split\n",
    "TRAIN_SPLIT = 0.9\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "# Knowledge Distillation Parameters\n",
    "TEMPERATURE = 4.0   # Temperature for softening distributions\n",
    "ALPHA = 0.3         # Weight for hard loss (1-alpha for soft loss)\n",
    "\n",
    "# Model Configuration\n",
    "TEACHER_MODEL_NAME = \"google/medsiglip-448\"\n",
    "STUDENT_MODEL_PATH = \"student_model_hw3.pt\"\n",
    "\n",
    "# Server Configuration\n",
    "SERVER_URL = 'http://hadi.cs.virginia.edu:8000'\n",
    "MY_TOKEN = 'your_token_here'  # Replace with your actual token\n",
    "\n",
    "# Performance Optimization\n",
    "USE_AMP = True  # Use Automatic Mixed Precision for faster training\n",
    "PIN_MEMORY = True  # Faster data transfer to GPU\n",
    "PERSISTENT_WORKERS = True  # Keep workers alive between epochs\n",
    "\n",
    "print(\"Configuration loaded ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Class and Function Definitions\n",
    "\n",
    "All classes and functions are defined here. Run these cells first before executing the main workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CLASS DEFINITIONS\n# ============================================================\n\nclass SkinDataset(Dataset):\n    \"\"\"Custom dataset for loading skin disease images.\"\"\"\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n        self.image_paths = []\n        self.labels = []\n        valid_exts = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.jfif')\n        for cls_name in self.classes:\n            cls_dir = os.path.join(root_dir, cls_name)\n            for fname in os.listdir(cls_dir):\n                if fname.lower().endswith(valid_exts):\n                    self.image_paths.append(os.path.join(cls_dir, fname))\n                    self.labels.append(self.class_to_idx[cls_name])\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\nclass DistillationLoss(nn.Module):\n    \"\"\"Knowledge distillation loss combining hard and soft losses.\"\"\"\n    def __init__(self, temperature=4.0, alpha=0.3):\n        super().__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n        self.ce_loss = nn.CrossEntropyLoss()\n    \n    def forward(self, student_logits, teacher_logits, labels):\n        \"\"\"\n        Compute knowledge distillation loss.\n        \n        Args:\n            student_logits: Raw logits from student model [batch_size, num_classes]\n            teacher_logits: Raw logits from teacher model [batch_size, num_classes]\n            labels: Ground truth labels [batch_size]\n        \n        Returns:\n            total_loss: Weighted combination of hard and soft losses\n            hard_loss: Cross-entropy loss with ground truth labels\n            soft_loss: KL divergence loss with teacher's soft targets\n        \"\"\"\n        # Hard loss: student predictions vs ground truth labels\n        hard_loss = self.ce_loss(student_logits, labels)\n        \n        # Soft loss: student predictions vs teacher predictions (temperature-scaled)\n        # Apply temperature scaling to soften the distributions\n        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)\n        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)\n        \n        # KL divergence loss (measures difference between distributions)\n        # Multiply by temperature^2 to scale the gradient properly (Hinton et al., 2015)\n        soft_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (self.temperature ** 2)\n        \n        # Combine losses: alpha * hard_loss + (1-alpha) * soft_loss\n        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n        \n        return total_loss, hard_loss, soft_loss\n\nprint(\"‚úì Classes defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA TRANSFORM FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def create_transforms():\n",
    "    \"\"\"Create training and validation transforms.\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_STD)\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_STD)\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "print(\"‚úì Transform functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL LOADING FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from torchvision.models import shufflenet_v2_x0_5\n",
    "\n",
    "def load_teacher_model():\n",
    "    \"\"\"Load MedSigLIP-448 teacher model from HuggingFace.\"\"\"\n",
    "    print(\"Loading MedSigLIP-448 teacher model...\")\n",
    "    \n",
    "    teacher_model = AutoModel.from_pretrained(TEACHER_MODEL_NAME, trust_remote_code=True)\n",
    "    processor = AutoProcessor.from_pretrained(TEACHER_MODEL_NAME, trust_remote_code=True)\n",
    "    \n",
    "    teacher_model = teacher_model.to(device)\n",
    "    teacher_model.eval()\n",
    "    \n",
    "    # Freeze all parameters\n",
    "    for param in teacher_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    print(\"‚úÖ MedSigLIP loaded successfully!\")\n",
    "    return teacher_model, processor\n",
    "\n",
    "\n",
    "def create_student_shufflenet(num_classes):\n",
    "    \"\"\"Create a ShuffleNetV2 student model (~5 MB).\"\"\"\n",
    "    model = shufflenet_v2_x0_5(pretrained=False)\n",
    "    # Replace final classifier\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "print(\"‚úì Model functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTIONS (OPTIMIZED)\n",
    "# ============================================================\n",
    "\n",
    "def get_teacher_logits(teacher, processor, images, class_names, normalize_mean, normalize_std):\n",
    "    \"\"\"\n",
    "    Get teacher logits from MedSigLIP using image-text similarity.\n",
    "\n",
    "    Args:\n",
    "        teacher: MedSigLIP model\n",
    "        processor: MedSigLIP processor\n",
    "        images: Batch of normalized tensors [B, 3, H, W]\n",
    "        class_names: List of class names\n",
    "        normalize_mean: Mean used for normalization\n",
    "        normalize_std: Std used for normalization\n",
    "\n",
    "    Returns:\n",
    "        logits: [B, num_classes] similarity scores\n",
    "    \"\"\"\n",
    "    # Denormalize images (reverse the normalization)\n",
    "    mean = torch.tensor(normalize_mean).view(1, 3, 1, 1).to(images.device)\n",
    "    std = torch.tensor(normalize_std).view(1, 3, 1, 1).to(images.device)\n",
    "    denormalized = images * std + mean\n",
    "\n",
    "    # Convert to PIL images (MedSigLIP processor expects PIL images)\n",
    "    denormalized = (denormalized * 255).clamp(0, 255).byte()\n",
    "    pil_images = [Image.fromarray(img.permute(1, 2, 0).cpu().numpy()) for img in denormalized]\n",
    "\n",
    "    # Create text prompts for each class\n",
    "    texts = [f\"a dermatology photo of {cls.replace('_', ' ')}\" for cls in class_names]\n",
    "\n",
    "    # Process inputs for MedSigLIP\n",
    "    inputs = processor(text=texts, images=pil_images, padding=\"max_length\", return_tensors=\"pt\").to(images.device)\n",
    "\n",
    "    # Get logits from teacher (image-text similarity scores)\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher(**inputs)\n",
    "        logits = outputs.logits_per_image  # [batch_size, num_classes]\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def train_epoch(student, teacher, teacher_proc, dataloader, criterion, optimizer, class_names,\n",
    "                normalize_mean=NORMALIZE_MEAN, normalize_std=NORMALIZE_STD, use_amp=USE_AMP):\n",
    "    \"\"\"Train for one epoch using knowledge distillation with optimizations.\"\"\"\n",
    "    student.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Create GradScaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp and torch.cuda.is_available() else None\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc='Training', leave=False):\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Get teacher predictions (no gradients, no AMP for teacher)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = get_teacher_logits(teacher, teacher_proc, images, class_names,\n",
    "                                                normalize_mean, normalize_std)\n",
    "\n",
    "        # Use automatic mixed precision for student (faster training)\n",
    "        with torch.cuda.amp.autocast(enabled=(use_amp and torch.cuda.is_available())):\n",
    "            # Get student predictions\n",
    "            student_logits = student(images)\n",
    "\n",
    "            # Compute distillation loss\n",
    "            loss, hard_loss, soft_loss = criterion(student_logits, teacher_logits, labels)\n",
    "\n",
    "        # Backpropagation with gradient scaling if using AMP\n",
    "        optimizer.zero_grad(set_to_none=True)  # set_to_none=True is faster than set_to_none=False\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(student, dataloader, use_amp=USE_AMP):\n",
    "    \"\"\"Validate the student model with optimizations.\"\"\"\n",
    "    student.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Validation', leave=False):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "\n",
    "            # Use mixed precision for faster inference\n",
    "            with torch.cuda.amp.autocast(enabled=(use_amp and torch.cuda.is_available())):\n",
    "                outputs = student(images)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return accuracy, f1\n",
    "\n",
    "print(\"‚úì Optimized training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SUBMISSION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def submit_model(token, model_path, server_url):\n",
    "    \"\"\"Submit model to the HW3 leaderboard.\"\"\"\n",
    "    with open(model_path, 'rb') as f:\n",
    "        files = {'file': f}\n",
    "        data = {'token': token}\n",
    "        response = requests.post(f'{server_url}/submit', data=data, files=files)\n",
    "        resp_json = response.json()\n",
    "        if 'message' in resp_json:\n",
    "            print(f\"‚úÖ {resp_json['message']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {resp_json.get('error', 'Unknown error')}\")\n",
    "\n",
    "\n",
    "def check_status(token, server_url):\n",
    "    \"\"\"Check your submission status.\"\"\"\n",
    "    url = f'{server_url}/submission-status/{token}'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        attempts = response.json()\n",
    "        for a in attempts:\n",
    "            score = f\"{a['score']:.4f}\" if isinstance(a['score'], (float, int)) else \"Pending\"\n",
    "            size = f\"{a['model_size']:.2f}\" if isinstance(a['model_size'], (float, int)) else \"N/A\"\n",
    "            print(f\"Attempt {a['attempt']}: Score={score}, Size={size} MB, Status={a['status']}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "\n",
    "print(\"‚úì Submission functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK TEST PIPELINE (for rapid testing)\n",
    "# ============================================================\n",
    "\n",
    "def quick_test_pipeline(\n",
    "    # Quick test parameters\n",
    "    num_samples=500,      # Use only 500 samples total\n",
    "    num_epochs=2,         # Just 2 epochs for quick testing\n",
    "    \n",
    "    # Use config defaults for everything else\n",
    "    dataset_path=DATASET_PATH,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    normalize_mean=NORMALIZE_MEAN,\n",
    "    normalize_std=NORMALIZE_STD,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_split=0.8,      # 80/20 split for quick test\n",
    "    temperature=TEMPERATURE,\n",
    "    alpha=ALPHA,\n",
    "    teacher_model_name=TEACHER_MODEL_NAME,\n",
    "    student_model_path=\"quick_test_model.pt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Quick test pipeline for rapid iteration and debugging.\n",
    "    Uses a small subset of data and fewer epochs.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Total number of samples to use (default: 500)\n",
    "        num_epochs: Number of training epochs (default: 2)\n",
    "        (other args same as run_training_pipeline)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results including best_f1, model_path, and model_size_mb\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"QUICK TEST PIPELINE (Small Dataset)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Using {num_samples} samples, {num_epochs} epochs\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load full dataset first\n",
    "    print(\"\\n[1/5] Loading dataset...\")\n",
    "    train_transform, val_transform = create_transforms()\n",
    "    full_dataset = SkinDataset(dataset_path, transform=train_transform)\n",
    "    \n",
    "    # Get class names for teacher model\n",
    "    class_names = full_dataset.classes\n",
    "    print(f'‚úÖ Classes detected: {class_names}')\n",
    "    \n",
    "    # Create a small subset\n",
    "    import random\n",
    "    indices = list(range(len(full_dataset)))\n",
    "    random.shuffle(indices)\n",
    "    subset_indices = indices[:num_samples]\n",
    "    \n",
    "    from torch.utils.data import Subset\n",
    "    dataset = Subset(full_dataset, subset_indices)\n",
    "    \n",
    "    print(f'‚úÖ Dataset subset created: {len(dataset)} images (from {len(full_dataset)} total)')\n",
    "    \n",
    "    # Load models\n",
    "    print(\"\\n[2/5] Loading models...\")\n",
    "    teacher_model, teacher_processor = load_teacher_model()\n",
    "    student_model = create_student_shufflenet(num_classes=num_classes).to(device)\n",
    "    print(f'‚úÖ Student model created: {sum(p.numel() for p in student_model.parameters()):,} parameters')\n",
    "    \n",
    "    # Setup training\n",
    "    print(\"\\n[3/5] Setting up training...\")\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    criterion = DistillationLoss(temperature=temperature, alpha=alpha)\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(f'‚úÖ Setup complete: Train={len(train_dataset)} | Val={len(val_dataset)}')\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\n[4/5] Training model ({num_epochs} epochs)...\")\n",
    "    print(\"-\"*70)\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('='*50)\n",
    "        \n",
    "        train_loss = train_epoch(student_model, teacher_model, teacher_processor, \n",
    "                                 train_loader, criterion, optimizer, class_names,\n",
    "                                 normalize_mean, normalize_std)\n",
    "        val_acc, val_f1 = validate(student_model, val_loader)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            print(f'‚úÖ New best F1: {best_f1:.4f}')\n",
    "    \n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Quick test complete! Best F1: {best_f1:.4f}')\n",
    "    print(f'{\"=\"*50}')\n",
    "    \n",
    "    # Save model\n",
    "    print(\"\\n[5/5] Saving model...\")\n",
    "    student_model.eval()\n",
    "    student_model.cpu()\n",
    "    scripted_model = torch.jit.script(student_model)\n",
    "    scripted_model.save(student_model_path)\n",
    "    \n",
    "    size_mb = os.path.getsize(student_model_path) / (1024 * 1024)\n",
    "    print(f'‚úÖ Model saved: {student_model_path}')\n",
    "    print(f'üì¶ Model size: {size_mb:.2f} MB')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö° Quick test finished! Use run_training_pipeline() for full training.\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return {\n",
    "        'best_f1': best_f1,\n",
    "        'model_path': student_model_path,\n",
    "        'model_size_mb': size_mb,\n",
    "        'train_size': len(train_dataset),\n",
    "        'val_size': len(val_dataset),\n",
    "        'num_samples': num_samples,\n",
    "        'num_epochs': num_epochs\n",
    "    }\n",
    "\n",
    "print(\"‚úì Quick test pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2B: Full Training Pipeline\n",
    "\n",
    "Run this cell for the complete training with all data and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with small dataset - runs in a few minutes\n",
    "test_results = quick_test_pipeline(\n",
    "    num_samples=500,    # Use only 500 images\n",
    "    num_epochs=2        # Just 2 epochs\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUICK TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best F1 Score: {test_results['best_f1']:.4f}\")\n",
    "print(f\"Model Size: {test_results['model_size_mb']:.2f} MB\")\n",
    "print(f\"Samples Used: {test_results['num_samples']}\")\n",
    "print(f\"Train/Val: {test_results['train_size']}/{test_results['val_size']}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ If this worked, you're ready to run the full pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2A: Quick Test (Optional)\n",
    "\n",
    "Run this cell first to quickly test that everything works before running the full training pipeline. This uses only 500 samples and 2 epochs, so it completes in a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "def run_training_pipeline(\n",
    "    # Dataset parameters\n",
    "    dataset_path='train_dataset',\n",
    "    num_classes=10,\n",
    "    \n",
    "    # Image processing parameters\n",
    "    image_size=224,\n",
    "    normalize_mean=[0.485, 0.456, 0.406],\n",
    "    normalize_std=[0.229, 0.224, 0.225],\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size=32,\n",
    "    num_epochs=10,\n",
    "    learning_rate=1e-3,\n",
    "    num_workers=2,\n",
    "    \n",
    "    # Data split parameters\n",
    "    train_split=0.9,\n",
    "    \n",
    "    # Distillation parameters\n",
    "    temperature=4.0,\n",
    "    alpha=0.3,\n",
    "    \n",
    "    # Model parameters\n",
    "    teacher_model_name=\"google/medsiglip-448\",\n",
    "    student_model_path=\"student_model_hw3.pt\",\n",
    "    \n",
    "    # Submission parameters\n",
    "    submit=False,\n",
    "    my_token='your_token_here',\n",
    "    server_url='http://hadi.cs.virginia.edu:8000'\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for knowledge distillation.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to training dataset directory\n",
    "        num_classes: Number of output classes\n",
    "        image_size: Size to resize images to (square)\n",
    "        normalize_mean: Mean values for normalization\n",
    "        normalize_std: Std values for normalization\n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        num_workers: Number of workers for data loading\n",
    "        train_split: Fraction of data to use for training (rest is validation)\n",
    "        temperature: Temperature for knowledge distillation\n",
    "        alpha: Weight for hard loss (1-alpha for soft loss)\n",
    "        teacher_model_name: HuggingFace model name for teacher\n",
    "        student_model_path: Path to save student model\n",
    "        submit: Whether to submit model to leaderboard (default: False)\n",
    "        my_token: Token for leaderboard submission\n",
    "        server_url: Server URL for submission\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results including best_f1, model_path, and model_size_mb\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"KNOWLEDGE DISTILLATION TRAINING PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ==================== STEP 1: LOAD DATASET ====================\n",
    "    print(\"\\n[1/5] Loading dataset...\")\n",
    "    train_transform, val_transform = create_transforms()\n",
    "    dataset = SkinDataset(dataset_path, transform=train_transform)\n",
    "    \n",
    "    # Get class names for teacher model\n",
    "    class_names = dataset.classes\n",
    "    \n",
    "    print(f'‚úÖ Dataset loaded: {len(dataset)} images, {len(dataset.classes)} classes')\n",
    "    print(f'‚úÖ Classes: {class_names}')\n",
    "    \n",
    "    # ==================== STEP 2: LOAD MODELS ====================\n",
    "    print(\"\\n[2/5] Loading models...\")\n",
    "    teacher_model, teacher_processor = load_teacher_model()\n",
    "    student_model = create_student_shufflenet(num_classes=num_classes).to(device)\n",
    "    print(f'‚úÖ Student model created: {sum(p.numel() for p in student_model.parameters()):,} parameters')\n",
    "    \n",
    "    # ==================== STEP 3: SETUP TRAINING ====================\n",
    "    print(\"\\n[3/5] Setting up training...\")\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    criterion = DistillationLoss(temperature=temperature, alpha=alpha)\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(f'‚úÖ Setup complete: Train={len(train_dataset)} | Val={len(val_dataset)}')\n",
    "    \n",
    "    # ==================== STEP 4: TRAIN MODEL ====================\n",
    "    print(\"\\n[4/5] Training model...\")\n",
    "    print(\"-\"*70)\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('='*50)\n",
    "        \n",
    "        # Train for one epoch with teacher guidance\n",
    "        train_loss = train_epoch(student_model, teacher_model, teacher_processor, \n",
    "                                 train_loader, criterion, optimizer, class_names,\n",
    "                                 normalize_mean, normalize_std)\n",
    "        \n",
    "        # Validate\n",
    "        val_acc, val_f1 = validate(student_model, val_loader)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            print(f'‚úÖ New best F1: {best_f1:.4f}')\n",
    "    \n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Training complete! Best F1: {best_f1:.4f}')\n",
    "    print(f'{\"=\"*50}')\n",
    "    \n",
    "    # ==================== STEP 5: SAVE MODEL ====================\n",
    "    print(\"\\n[5/5] Saving model...\")\n",
    "    student_model.eval()\n",
    "    student_model.cpu()\n",
    "    scripted_model = torch.jit.script(student_model)\n",
    "    scripted_model.save(student_model_path)\n",
    "    \n",
    "    size_mb = os.path.getsize(student_model_path) / (1024 * 1024)\n",
    "    print(f'‚úÖ Model saved: {student_model_path}')\n",
    "    print(f'üì¶ Model size: {size_mb:.2f} MB')\n",
    "    \n",
    "    if size_mb >= 25.0:\n",
    "        print('‚ùå WARNING: Model exceeds 25 MB limit!')\n",
    "    else:\n",
    "        print('‚úÖ Model size is within the 25 MB limit')\n",
    "    \n",
    "    # ==================== OPTIONAL: SUBMIT ====================\n",
    "    if submit:\n",
    "        print(\"\\n[BONUS] Submitting model to leaderboard...\")\n",
    "        submit_model(my_token, student_model_path, server_url)\n",
    "        check_status(my_token, server_url)\n",
    "    else:\n",
    "        print(f'\\nüí° To submit, set submit=True or run manually:')\n",
    "        print(f'   submit_model(\"{my_token}\", \"{student_model_path}\", \"{server_url}\")')\n",
    "    \n",
    "    print(f'\\nüéØ View the HW3 leaderboard at: {server_url}/leaderboard3')\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'best_f1': best_f1,\n",
    "        'model_path': student_model_path,\n",
    "        'model_size_mb': size_mb,\n",
    "        'train_size': len(train_dataset),\n",
    "        'val_size': len(val_dataset)\n",
    "    }\n",
    "\n",
    "print(\"‚úì Main pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Run Training Pipeline\n",
    "\n",
    "Simply call the `run_training_pipeline()` function with your desired parameters.\n",
    "All parameters use the configuration values defined in Cell 2 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete training pipeline using config values\n",
    "results = run_training_pipeline(\n",
    "    # Dataset parameters\n",
    "    dataset_path=DATASET_PATH,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    \n",
    "    # Image processing parameters\n",
    "    image_size=IMAGE_SIZE,\n",
    "    normalize_mean=NORMALIZE_MEAN,\n",
    "    normalize_std=NORMALIZE_STD,\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    \n",
    "    # Data split parameters\n",
    "    train_split=TRAIN_SPLIT,\n",
    "    \n",
    "    # Distillation parameters\n",
    "    temperature=TEMPERATURE,\n",
    "    alpha=ALPHA,\n",
    "    \n",
    "    # Model parameters\n",
    "    teacher_model_name=TEACHER_MODEL_NAME,\n",
    "    student_model_path=STUDENT_MODEL_PATH,\n",
    "    \n",
    "    # Submission parameters (set submit=True to auto-submit)\n",
    "    submit=False,  # Change to True to submit automatically\n",
    "    my_token=MY_TOKEN,\n",
    "    server_url=SERVER_URL\n",
    ")\n",
    "\n",
    "# Display final results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best F1 Score: {results['best_f1']:.4f}\")\n",
    "print(f\"Model Path: {results['model_path']}\")\n",
    "print(f\"Model Size: {results['model_size_mb']:.2f} MB\")\n",
    "print(f\"Training Set: {results['train_size']} samples\")\n",
    "print(f\"Validation Set: {results['val_size']} samples\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}