EXECUTION ORDER FOR HOMEWORK 3: KNOWLEDGE DISTILLATION
========================================================

This document outlines the order in which to run the notebook cells for the
AI Dermatologist knowledge distillation project.

The notebook is now organized into TWO main parts:
- PART 1: Class and Function Definitions (run once)
- PART 2: Main Execution (can be run multiple times for experimentation)

================================================================================
PART 1: CLASS AND FUNCTION DEFINITIONS (Run these first, in order)
================================================================================

CELL 1: IMPORTS & DEVICE SETUP
-------------------------------
- Import all necessary libraries (PyTorch, transformers, sklearn, etc.)
- Detect and set up device (CPU/CUDA)
- MUST RUN FIRST

CELL 2: CONFIGURATION
---------------------
- Define all tunable parameters in one place
- Dataset paths, image size, normalization values
- Training hyperparameters (batch size, epochs, learning rate)
- Distillation parameters (temperature, alpha)
- Model paths and server configuration
- MUST RUN SECOND (before any other cells that use these variables)

CELL 3: CLASS DEFINITIONS
--------------------------
- SkinDataset: Custom dataset for loading skin disease images
- DistillationLoss: Knowledge distillation loss combining hard and soft losses
- Dependencies: Requires CELL 1 (imports)

CELL 4: DATA TRANSFORM FUNCTIONS
---------------------------------
- create_transforms(): Creates training and validation transforms
- Uses IMAGE_SIZE, NORMALIZE_MEAN, NORMALIZE_STD from config
- Dependencies: Requires CELL 1, CELL 2

CELL 5: MODEL LOADING FUNCTIONS
--------------------------------
- load_teacher_model(): Loads MedSigLIP-448 from HuggingFace
- create_student_shufflenet(): Creates ShuffleNetV2 student model
- Dependencies: Requires CELL 1, CELL 2

CELL 6: TRAINING FUNCTIONS
---------------------------
- train_epoch(): Trains for one epoch using knowledge distillation
- validate(): Validates the student model
- Dependencies: Requires CELL 1, CELL 2

CELL 7: SUBMISSION FUNCTIONS
-----------------------------
- submit_model(): Submits model to the HW3 leaderboard
- check_status(): Checks submission status
- Dependencies: Requires CELL 1

================================================================================
PART 2: MAIN EXECUTION (Run these in order to execute the pipeline)
================================================================================

CELL 8: LOAD DATASET
--------------------
- Create transforms using create_transforms()
- Load dataset using SkinDataset class
- Verify dataset loaded correctly
- Dependencies: Requires CELLS 1-4

CELL 9: LOAD MODELS
-------------------
- Load MedSigLIP teacher model using load_teacher_model()
- Create student model using create_student_shufflenet()
- Initialize models on device
- Dependencies: Requires CELLS 1-5
- NOTE: First run downloads teacher model (~1-2 GB)

CELL 10: SETUP TRAINING
------------------------
- Split dataset into train/validation sets using TRAIN_SPLIT
- Create DataLoaders with BATCH_SIZE and NUM_WORKERS
- Create DistillationLoss criterion with TEMPERATURE and ALPHA
- Set up Adam optimizer with LEARNING_RATE
- Dependencies: Requires CELLS 1-9

CELL 11: TRAIN MODEL
--------------------
- Train for NUM_EPOCHS epochs
- Uses train_epoch() and validate() functions
- Track and display best F1 score
- Dependencies: Requires CELLS 1-10
- NOTE: This cell takes the longest to run

CELL 12: SAVE & SUBMIT MODEL
-----------------------------
- Save trained student model to STUDENT_MODEL_PATH
- Check model size (must be < 25 MB)
- Display submission instructions
- Dependencies: Requires CELLS 1-11
- NOTE: Uncomment submission lines to actually submit

SUMMARY OF EXECUTION FLOW
==========================
PART 1 (Run once):
1. Import libraries and setup device
2. Load configuration variables
3. Define SkinDataset and DistillationLoss classes
4. Define transform creation function
5. Define model loading functions
6. Define training functions
7. Define submission functions

PART 2 (Main workflow):
8. Load and prepare dataset
9. Load teacher model and create student model
10. Setup training (data loaders, criterion, optimizer)
11. Train the student model using knowledge distillation
12. Save and optionally submit the trained model

IMPORTANT NOTES
===============
- PART 1 cells (1-7) define everything - run these ONCE at the start
- PART 2 cells (8-12) execute the workflow - run these in order
- Do NOT skip cells, as later cells depend on earlier ones
- If you modify CELL 2 (configuration), restart kernel and rerun all cells
- After running PART 1 once, you can rerun PART 2 multiple times

TYPICAL WORKFLOW FOR EXPERIMENTATION
=====================================
Initial Setup (run once):
1. Run PART 1: CELLS 1-7 to define all classes and functions

First Training Run:
2. Run PART 2: CELLS 8-12 to execute the complete pipeline

Experimentation (repeat as needed):
3. Modify CELL 2 configuration (e.g., BATCH_SIZE, LEARNING_RATE, TEMPERATURE, ALPHA)
4. Re-run CELLS 9-12 (skip dataset loading if using same data)
5. Compare results and iterate

ADVANTAGES OF THIS ORGANIZATION
================================
- Clear separation between definitions and execution
- All functions defined in one place for easy reference
- Can rerun execution cells without redefining functions
- Easier to understand the overall workflow
- Better software engineering practice

TROUBLESHOOTING
===============
- If you get "NameError: name 'X' is not defined", you skipped a cell
- If training is too slow, reduce BATCH_SIZE or NUM_WORKERS in CELL 2
- If model is > 25 MB, try a smaller student model architecture
- If you run out of memory, reduce BATCH_SIZE or use CPU instead of CUDA
