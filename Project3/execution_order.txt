EXECUTION ORDER FOR HOMEWORK 3: KNOWLEDGE DISTILLATION
========================================================

This document outlines the order in which to run the notebook cells for the
AI Dermatologist knowledge distillation project.

The notebook is now organized into TWO main parts:
- PART 1: Class and Function Definitions (run once)
- PART 2: Main Execution (can be run multiple times for experimentation)

================================================================================
PART 1: CLASS AND FUNCTION DEFINITIONS (Run these first, in order)
================================================================================

CELL 1: IMPORTS & DEVICE SETUP
-------------------------------
- Import all necessary libraries (PyTorch, transformers, sklearn, etc.)
- Detect and set up device (CPU/CUDA)
- MUST RUN FIRST

CELL 2: CONFIGURATION
---------------------
- Define all tunable parameters in one place
- Dataset paths, image size, normalization values
- Training hyperparameters (batch size, epochs, learning rate)
- Distillation parameters (temperature, alpha)
- Model paths and server configuration
- MUST RUN SECOND (before any other cells that use these variables)

CELL 3: CLASS DEFINITIONS
--------------------------
- SkinDataset: Custom dataset for loading skin disease images
- DistillationLoss: Knowledge distillation loss combining hard and soft losses
- Dependencies: Requires CELL 1 (imports)

CELL 4: DATA TRANSFORM FUNCTIONS
---------------------------------
- create_transforms(): Creates training and validation transforms
- Uses IMAGE_SIZE, NORMALIZE_MEAN, NORMALIZE_STD from config
- Dependencies: Requires CELL 1, CELL 2

CELL 5: MODEL LOADING FUNCTIONS
--------------------------------
- load_teacher_model(): Loads MedSigLIP-448 from HuggingFace
- create_student_shufflenet(): Creates ShuffleNetV2 student model
- Dependencies: Requires CELL 1, CELL 2

CELL 6: TRAINING FUNCTIONS
---------------------------
- train_epoch(): Trains for one epoch using knowledge distillation
- validate(): Validates the student model
- Dependencies: Requires CELL 1, CELL 2

CELL 7: SUBMISSION FUNCTIONS
-----------------------------
- submit_model(): Submits model to the HW3 leaderboard
- check_status(): Checks submission status
- Dependencies: Requires CELL 1

CELL 8: MAIN TRAINING PIPELINE FUNCTION
-----------------------------------------
- Define run_training_pipeline() function
- Encapsulates all 5 steps: Load Data, Load Models, Setup, Train, Save
- Takes all config parameters as function arguments
- Includes submit parameter (default=False) for optional auto-submission
- Returns dict with results (best_f1, model_path, model_size_mb, etc.)
- Dependencies: Requires CELLS 1-7

================================================================================
PART 2: MAIN EXECUTION (Run this single cell to execute everything)
================================================================================

CELL 9: RUN COMPLETE PIPELINE
------------------------------
- Single function call: run_training_pipeline()
- Passes all configuration values from CELL 2 as arguments
- Set submit=False (default) to save model without submitting
- Set submit=True to automatically submit after training
- Prints final results summary at the end
- Dependencies: Requires ALL previous cells (1-8)
- NOTE: This cell executes all 5 steps automatically:
  [1/5] Load dataset
  [2/5] Load models (downloads teacher model ~1-2 GB on first run)
  [3/5] Setup training
  [4/5] Train model (this takes the longest)
  [5/5] Save model (and optionally submit if submit=True)

SUMMARY OF EXECUTION FLOW
==========================
PART 1 (Run once - Cells 1-9):
1. Import libraries and setup device
2. Load configuration variables
3. Define SkinDataset and DistillationLoss classes
4. Define transform creation function
5. Define model loading functions
6. Define training functions
7. Define submission functions
8. Define main pipeline function (run_training_pipeline)

PART 2 (Main execution - Cell 10):
9. Call run_training_pipeline() with all config parameters
   - Automatically runs all 5 steps internally
   - Returns results dictionary
   - Optionally submits if submit=True

IMPORTANT NOTES
===============
- PART 1 cells (1-9) define everything - run these ONCE at the start
- PART 2 is just ONE cell (10) - calls the pipeline function
- Do NOT skip cells, as later cells depend on earlier ones
- If you modify CELL 2 (configuration), you can just rerun CELL 10
- After running PART 1 once, you can rerun CELL 10 with different parameters

TYPICAL WORKFLOW FOR EXPERIMENTATION
=====================================
Initial Setup (run once):
1. Run PART 1: CELLS 1-9 to define all classes and functions

First Training Run:
2. Run CELL 10 to execute the complete pipeline
3. Review results printed at the end

Quick Experimentation (fastest way):
4. Rerun CELL 10 with modified parameters directly in the function call
   Example: Change num_epochs=5, temperature=3.0 in the function arguments
5. Compare results

Full Config Experimentation:
6. Modify CELL 2 configuration (e.g., BATCH_SIZE, LEARNING_RATE, TEMPERATURE, ALPHA)
7. Rerun CELL 10 (it will use the updated config values)
8. Compare results and iterate

Submission:
9. When ready, set submit=True in CELL 10 or call submit_model() manually

ADVANTAGES OF THIS ORGANIZATION
================================
- Clear separation between definitions and execution
- All functions defined in one place for easy reference
- Single function encapsulates entire workflow - very easy to use
- Can modify parameters without changing multiple cells
- Easy to experiment: just rerun one cell with different parameters
- Can call the pipeline function multiple times with different configs
- Clean code structure following software engineering best practices
- submit parameter makes submission optional and explicit (default: False)

TROUBLESHOOTING
===============
- If you get "NameError: name 'X' is not defined", you skipped a cell
- If training is too slow, reduce BATCH_SIZE or NUM_WORKERS in CELL 2
- If model is > 25 MB, try a smaller student model architecture
- If you run out of memory, reduce BATCH_SIZE or use CPU instead of CUDA
